import numpy as np
import argparse
import torch
import sys
import enum
import littlemcmc as lmc
import time
import scipy as sp
import matplotlib.pyplot as plt
import scipy.stats as sps
import pickle
import os

from bnn.bnn_pytorch import BayesianNeuralNet, ActivationFunctions
from bnn.bnn_utils import Layers, BNNParameterScale, ParamStdDistr
from deconvolution_1d.deconv_bnn import ObservationData, SignalType, NoiseLevel

from utils.plot_solution import plot_solution, plot_chain_statistics_together, plot_tracking_points
from student_bnn.prob_for_nuts import NUTSForBNN
from utils.quality_metrics import compute_rel_error


class SamplerStart(str, enum.Enum):
    """
    Specifies the starting point for the NUTS algorithm
    "map": start from the MAP estimate (recommended option),
           MAP should be precomputed (use main_deconv_1d_compute_map.py)
    "prior": start from a sample randomly generated from the prior distribution (i.e. Gaussian)
    """
    map = "map"
    prior = "prior"


N_DATA = 128
N_DISCR = 130


RES_PATH_CONST = "results/"  # path for saving the results
DATA_PATH_CONST = "deconvolution_1d/data/"  # path to the data

# parameters specifying domain boundaries
LOWER_BOUND = 0
UPPER_BOUND = 1

# how many chains we would like to track
N_IND_PLOT = 5

# specify the noise level
NOISE = NoiseLevel.low

# activation function for the hidden layers in BNN
act_func = ActivationFunctions.TANH

# list defining the size of input as a first entity and  number of nodes in the hidden layers
layer_vector = [1, 40, 80, 1]


def main():
    np.random.seed(1996)

    # specify arguments
    parser = argparse.ArgumentParser(description="Process command line " "arguments")

    parser.add_argument(
        "--start",
        "-s",
        dest="sampler_start",
        choices=[
            SamplerStart.map.value,
            SamplerStart.prior.value,
        ],
        default=SamplerStart.map,
        type=SamplerStart,
        help="Starting point for the NUTS sampler",
    )

    parser.add_argument(
        "--problem",
        "-p",
        dest="signal_type",
        choices=[
            SignalType.smooth.value,
            SignalType.datasky.value,
        ],
        default=SignalType.smooth,
        type=SignalType,
        help="Type of the problem/signal: smooth or sharp",
    )

    parser.add_argument(
        "--std",
        "-d",
        dest="std_distr",
        choices=[
            ParamStdDistr.exp.value,
            ParamStdDistr.const.value,
        ],
        default=ParamStdDistr.exp,
        type=ParamStdDistr,
        help="Does std for the BNN parameters follow some distribution (e.g. exponential) or is it constant?",
    )

    parser.add_argument(
        "--n_smpl",
        "-ns",
        dest="n_calls",
        default=10,  # int(2e4),
        type=int,
        help="Number of states to be generated by NUTS",
    )

    parser.add_argument(
        "--n_burn",
        "-nb",
        dest="n_burn",
        default=10,  # int(5e3),
        type=int,
        help="Length of the burn-in period for NUTS",
    )

    args = parser.parse_args()

    # for simplicity, we rename parameters that are used frequently
    n_calls = args.n_calls
    n_burn = args.n_burn

    print(f"Signal type: {args.signal_type.value}")
    print(f"BNN activation function: {act_func}")
    print(f"BNN architecture: {layer_vector}")
    print(f"Initial std of the BNN parameters: {args.std_distr.value}")
    print(f"Starting point for NUTS: {args.sampler_start.value}")

    # handle the info on the dimensionality of a problem
    try:
        layers = Layers(values=layer_vector)
    except ValueError as e:
        print(str(e))
        sys.exit(1)

    # set the parameter of the alpha-stable distribution
    signal_type = args.signal_type
    if signal_type == SignalType.smooth:
        alpha = 2
    elif signal_type == SignalType.datasky:
        alpha = 1
    else:
        raise ValueError("Wrong value of alpha (parameter of alpha-stable distribution, should be 1 or 2)")

    # set the data path common for all the samplers
    res_path = (
        RES_PATH_CONST
        + f"{signal_type.value}_signal_{args.std_distr.value}_std/"
        + f'{act_func}_{"_".join(str(x) for x in layers.hidden_layer_dimensions)}_nodes/'
    )

    # MAP estimate file name
    map_file_name = res_path + "map_estimates.pkl"

    # path to save the results produced by NUTS
    sampler_data_path = res_path + f"nuts_{args.sampler_start.value}_start_{n_calls}_calls_{n_burn}_burn/"
    res_filename = sampler_data_path + "nuts_results.pkl"

    # path for figures
    fig_path = sampler_data_path + "figures/"
    # create the path if it does not exist
    if not os.path.exists(fig_path):
        os.makedirs(fig_path)

    # load observations from a  file
    data_file_name = DATA_PATH_CONST + f"{signal_type.value}_m_{N_DATA}_n_{N_DISCR}_{NOISE.value}.pkl"
    obs_data = ObservationData(data_file_name, signal_type)

    # dimensionality of the parameter vector
    param_dim = layers.param_vector_dimension

    # input vector (discretization of the  interval [LOWER_BOUND, UPPER_BOUND])
    x_input = torch.tensor(np.linspace(LOWER_BOUND, UPPER_BOUND, N_DISCR), dtype=torch.double).view(1, -1)

    # set the initial standard deviation for the BNN parameters
    if args.std_distr == ParamStdDistr.exp:
        # use random stds for weights and biases in all layers
        std_weights_vec = sps.expon.rvs(loc=0, scale=5, size=layers.num_transforms)
        std_biases_vec = sps.expon.rvs(loc=0, scale=4, size=layers.num_transforms)
    elif args.std_distr == ParamStdDistr.const:
        # use constant value 1 for all the layers
        std_weights_vec = 1
        std_biases_vec = 1
    else:
        raise ValueError("Wrong value of the distribution for the BNN parameter std")

    # create an instance of the class managing the BNN parameter scales
    param_scale = BNNParameterScale(
        layers,
        alpha,
        mu_weights=0,
        mu_biases=0,
        std_weights=std_weights_vec,
        std_biases=std_biases_vec,
    )

    # initialize parameter vector based on the type of the starting point in NUTS
    if args.sampler_start == SamplerStart.map:

        # check that MAP estimate file exists
        if not os.path.exists(map_file_name):
            raise ValueError("MAP file does not exist yet. Please run main_deconv_1d_compute_map.py to get MAP")

        # load MAP
        with open(map_file_name, "rb") as f:
            ksi_map_s, theta_map_s, u_map_s = pickle.load(f)

        # find out how many MAP estimates are stored in the file
        num_maps = ksi_map_s.shape[0]

        # choose "the best" MAP estimate = the one that provides the smaller relative error
        relerr_all = np.zeros(num_maps)
        for i in range(num_maps):
            u_map_i = u_map_s[i]
            # compute the relative error
            relerr_all[i] = compute_rel_error(obs_data.signal_true, u_map_i)

        min_relerr_ind = np.argmin(relerr_all)
        print(f"MAP {min_relerr_ind+1} delivers the smallest relative error")

        # initialize:  ksi_0 = MAP, MAP is chosen based on the relerr
        ksi_0 = ksi_map_s[min_relerr_ind, :]

    elif args.sampler_start == SamplerStart.prior:
        # initialize: ksi is standard Gaussian
        ksi_0 = np.random.randn(param_dim)

    else:
        raise ValueError("Wrong value of the starting point: should be 'map' or 'prior'")

    # set random seed if needed
    n_seed = np.random.seed(5)

    # number of chains for NUTS to generate
    n_chains = 1

    # total number of samples for NUTS to generate
    n_total = int(n_calls + n_burn)

    # create an instance of the class containing likelihood, posterior and potential for our problem
    nuts_class = NUTSForBNN(x_input, obs_data, act_func, layers, param_scale)

    # run NUTS to generate a chain approximating the solution
    step = lmc.NUTS(logp_dlogp_func=nuts_class.log_posterior, model_ndim=param_dim, max_treedepth=10)
    tic = time.time()
    chains, stats = lmc.sample(
        logp_dlogp_func=nuts_class.log_posterior,
        model_ndim=param_dim,
        step=step,
        draws=n_calls,
        tune=n_burn,
        chains=n_chains,
        cores=n_chains,
        start=ksi_0,
        random_seed=n_seed,
        target_accept=0.8,
        discard_tuned_samples=False,
        progressbar=True,
    )
    toc = time.time() - tic
    print("\nElapsed time sampling\n:", toc / 60)

    # store results
    ksi_s = chains[0]

    # FIRST we remove the burn-in for ksi
    ksi_s = ksi_s[n_burn:, :]

    # based on the obtained ksi values we compute theta (by applying the transform)
    # and u (by performing the forward pass of BNN with parameters theta)
    u_chain = np.empty((n_calls, N_DISCR))
    theta_s = np.array([param_scale.transform_to_theta(ksi_s[i, :])[0] for i in range(n_calls)])

    for i in range(n_calls):
        theta = torch.tensor(theta_s[i, :], dtype=torch.double)
        bnn = BayesianNeuralNet(act_func, layers, theta)
        u = bnn(x_input)
        u_chain[i, :] = u.view(-1).detach().numpy()

    # save the NUTS results
    with open(res_filename, "wb") as f:
        pickle.dump([ksi_s, theta_s, u_chain, n_calls, n_burn, n_seed, toc / 3600], f)

    # # load NUTS results (this block is needed if the NUTS results are already computed and we need just to load them)
    # with open(res_filename, "rb") as f:
    #     res = pickle.load(f)
    # ksi_s = res[0][n_burn:, :]
    # theta_s = res[1][n_burn:, :]
    # u_chain = res[2]

    if args.sampler_start == SamplerStart.map:
        plot_solution(u_chain, fig_path, obs_data, [LOWER_BOUND, UPPER_BOUND], u_map_s[min_relerr_ind])
    elif args.sampler_start == SamplerStart.prior:
        plot_solution(u_chain, fig_path, obs_data, [LOWER_BOUND, UPPER_BOUND])
    else:
        raise ValueError("Wrong value of the starting point: should be 'map' or 'prior'")

    # plot chain statistics for five chosen points of u
    if signal_type == SignalType.smooth:
        ind_u = [20, 50, 77, 93, 105]

    elif signal_type == SignalType.datasky:
        ind_u = [13, 28, 48, 65, 103]
    else:
        raise ValueError("Wrong signal type: must be 'smooth' or 'datasky'")

    # plot points u(i) whose chains we track
    plot_tracking_points(obs_data, ind_u, [LOWER_BOUND, UPPER_BOUND], fig_path)

    print(f"Indices of u to plot: {ind_u}")
    domain_points = obs_data.domain_discr[ind_u]
    print(f"Correspond to x-points: {domain_points}")
    u_plot = u_chain[:, ind_u]

    # plot chain statistics for n_ind_plot chosen points of theta
    n_ind_plot = N_IND_PLOT
    ind_theta = np.random.choice(np.arange(0, theta_s.shape[1]), n_ind_plot, replace=False)
    ind_ksi = ind_theta
    theta_plot = theta_s[:, ind_theta]

    # plot chain statistics for n_ind_plot chosen points of ksi
    ksi_plot = ksi_s[:, ind_ksi]

    # plot chain statistics for all the parameters (u, theta and ksi) in one figure
    plot_chain_statistics_together(u_plot, theta_plot, ksi_plot, fig_path)

    plt.show()

    return


if __name__ == "__main__":
    main()
